# Overview and Architecture 

*Generated by AI*

## Project Overview

This is a Spring Boot WebFlux project using Reactive Kafka (reactor-kafka) for performance and load testing different Kafka streaming patterns. The application demonstrates various producer and consumer topologies with a focus on reactive streams and performance benchmarking.

**Key Technologies:**
- Spring Boot 3.5.7 with WebFlux
- Project Reactor & Reactor Kafka 1.3.24
- Java 17
- Redis for caching/deduplication
- Docker/Testcontainers for testing
- Prometheus/Grafana/Loki for observability

## Architecture

### Application Modes
The application runs in different modes controlled by the `APPLICATION_MODE` environment variable:

**Producer modes:** Generate data to Kafka topics
- `ProduceFlatMapService` - Uses `flatMap` for concurrent sending
- `ProduceConcatMapService` - Uses `concatMap` for sequential sending
- `ProduceSendSourceService` - Uses `send(source())` pattern
- `ProduceTransactionalService` - Transactional producer
- `ProduceWithDuplicatesService` - Producer with configurable duplicates

**Pipe modes:** Transform data between topics (A → B)
- `PipeReceiveSend` - `receive().flatMap(r -> send(transform(r)))`
- `PipeSendReceive` - `send(receive().map(r -> transform(r)))`
- `PipePartitioned` - Partitioned processing with manual commits
- `PipeExactlyOnce` - Exactly-once delivery semantics
- `PipeDedup` - Deduplication using Redis cache

**Consumer mode:**
- `Consume` - Consumes and logs messages

### Key Service Architecture
```
com.giraone.kafka.pipeline.service/
├── AbstractService.java          # Base service with metrics
├── produce/                      # Producer implementations
│   ├── AbstractProduceService.java
│   └── [Various producer services]
├── pipe/                         # Stream processing
│   ├── AbstractPipeService.java
│   └── [Various pipe services]
├── consume/
│   └── ConsumeService.java
└── CounterService.java          # Metrics collection
```

### Configuration Structure
- `ApplicationProperties.java` - Main configuration with nested properties
- `KafkaProducerConfig.java` & `KafkaConsumerConfig.java` - Kafka-specific configs
- Mode-specific property classes in `config.properties/`
- Application configs: `application.yml`, `application-docker.yml`, `application-cloud.yml`

## Development Commands

### Build & Test
```bash
# Build with all profiles
mvn clean compile

# Run tests (uses Testcontainers for Kafka)
mvn test

# Build with Loki profile and skip tests
mvn -Ploki package -DskipTests

# Build Docker image
./dockerize.sh
# OR: mvn -ntp verify -DskipTests -Ploki jib:dockerBuild
```

### Running Locally
```bash
# Run different modes locally:
./produce.sh [mode] [topic] [interval] [maxEvents] [duplicatePercentage]
./pipe.sh [mode] [topicA] [topicB] [groupId] [processingTime] [schedulerType] [maxPollRecords] [maxPollInterval]
./consume.sh [mode] [groupId] [topic]
./dedup.sh   # Runs PipeDedup mode

# Default modes:
./produce.sh  # ProduceFlatMapService to topic a1, 1ms interval, 100k events
./pipe.sh     # PipeReceiveSend from a1 to b1
./consume.sh  # Consumes from b1
```

### Docker Environment
```bash
cd docker

# Start all services (Kafka, Redis, observability stack + app containers)
docker-compose up -d

# Start only infrastructure (when running apps locally)
docker-compose -f docker-compose-subsystems-only.yml up -d
./kafka-create-topics.sh

# Redis cluster option
docker-compose -f docker-compose-redis.yml up -d
```

### Testing Individual Services
```bash
# Run specific test class
mvn test -Dtest=ProduceFlatMapServiceIntTest

# Run all integration tests for a service type
mvn test -Dtest="**/*ServiceIntTest"

# Test with specific Kafka configuration
mvn test -Dtest=PipeReceiveSendServiceIntTest -Dspring.config.location=src/test/resources/pipe/test-pipe-receive-send.properties
```

## Key Configuration Patterns

### Application Mode Selection
Set `APPLICATION_MODE` environment variable to one of the service class names (without "Service" suffix).

### Kafka Configuration
- Topics: `application.topic-a` (default: a1), `application.topic-b` (default: b1)
- Consumer settings: `application.consumer.*` (max-poll-records, max-poll-interval, etc.)
- Producer settings: `application.producer.*`

### Performance Tuning Properties
```yaml
application:
  produce-interval: 1ms          # Producer event interval
  processing-time: 10ms          # Pipe processing simulation time
  consumer:
    max-poll-records: 500        # Kafka consumer batch size
    max-poll-interval: 300s      # Consumer session timeout
  producer-variables:
    max-number-of-events: 1000000
    duplicate-percentage: 0.0    # For testing dedup scenarios
```

## Observability

- **Metrics**: Exposed via `/actuator/prometheus` endpoint
- **Logging**: Structured logging with Loki integration (`-Dspring.profiles.active=loki`)
- **Custom Metrics**: CounterService tracks throughput and processing metrics
- **Grafana Dashboards**: Preconfigured in `docker/grafana/`

## Testing Approach

Integration tests use Testcontainers to spin up real Kafka brokers. Test configurations are in `src/test/resources/[consume|pipe|produce]/test-*.properties`.

Key test patterns:
- `AbstractKafkaIntTest` - Base class with Kafka container setup
- Service-specific integration tests verify throughput and correctness
- KafkaBrokerCli utility for manual topic manipulation during tests